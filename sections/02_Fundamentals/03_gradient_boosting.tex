\subsection{Decision Trees}

Decision Trees are one of the most widely used supervised Machine Learning Algorithms either 
as standalone solutions or in combination with enhancement approaches like Boosting. They are 
furthermore very flexible in their construction and can be used for various Machine Learning 
Problems such as Classification and Regression. 

Decision Trees “predict an unknown value of a target variable by learning decision rules from 
data features” to reconstruct the dependence between the features and the respective labels for 
each sample. To perform Classification or Regression, Decision Trees rely on recursive 
splitting of the dataset into multiple subgroups. As the number of iterations increases, the 
subgroups become more and more homogeneous \cite[p.330]{James2021}. The ideal result is that each subgroup is fully 
homogeneous and therefore only represents a single category (in case of Classification). However, 
this is often only a theoretical best condition, as multiple risks, such as overfitting, are 
associated with the increasing depth of Decision Trees.

Trees consist out of four main components. A Node is a discrete Decision Function that takes 
samples as its input and splits them based on features into subgroups. The aim of each split, 
as previously discussed, is to create a split that results in the overall most homogeneous 
distribution for all subgroups \cite[p.6]{lewis2000introduction}. Nodes can be subclassified into three kinds. The Top-Node, 
from which the classification starts, is called a Root Node. Nodes that are located at the 
very end of a Decision Tree are referred to as Leaves. Leaves do not split data any further and 
only mark the end of a Decision Tree. When reached, Leaves categorize or predict a final output 
value depending on the prediction task. Nodes in-between the Root Node and Leaves are called 
Internal Leaves. Like the Root, Internal Leaves are responsible for the recursive splitting of 
the data. Branches connect Nodes with another. For classical Trees, information only flows from 
the top to the bottom of the Tree. 

\subsubsection{Decision Tree Algorithm}

In practice, there exist various Algorithms for computing Decision Trees with the most common ones 
being: ID3, C4.5, C5.0 and CART. Each algorithm follows the same principle of regressively finding 
perfect splits to separate data but utilizes different methods to find the ideal splitting 
criteria, which strongly influences the structure of the Tree, its accuracy and performance. 
Additionally, each Algorithm has its benefits and constraints. Therefore, it is important to 
determine the best Algorithm before implementing a Decision Tree based on the prediction task 
and dataset. This project uses the Python library Sklearn to implement a Classification Tree. 
Sklearn is based on the CART Algorithm \cite[10.10.6]{sklearn Decision Trees}.

To better visualize the procedure of the Decision Tree Algorithm, a simplified dataset is used, 
on which the individual steps are explained. For this example, a Classification Problem is chosen. 
The dataset consists out of actual features and labels from the project implementation phase. The 
features are “acousticness” and “danceability”. Both features are numerical with a value range 
in-between \(0\) and \(0\). The Classification Problem is binary with "Hiphop" and "Jazz" representing 
the classes \(k\) for which the samples of the dataset are classified. Mathematically, the dataset 
is represented in the following form: \(x_{i}\) present the explanatory features while \(y_{i}\) represents 
the corresponding label for one data point of the input dataset \(N\) with a total number of \(n\) 
samples.

(TABLE WITH DATA)

(COORDINATE SYSTEM WITH DATA)

With the initialization of the dataset complete, the implementation of the Decision Tree 
Algorithm can begin. The Decision Tree Algorithm splits a Node represented by \(Q_{m}\) with \(N_{m}\) samples 
into multiple subgroups. The split can be binary, which means that \(Q_{m}\) is divided into two 
subgroups \(Q^{left}_{m}\) and \(Q^{right}_{m}\), or multiway. While multiway splitting seems to be more advanced 
with greater prediction potential, in practice and for CART binary splitting is used almost 
exclusively (QUELLE). The split \(\theta = (j, t_{m})\) consists out of a feature \(j\) and a threshold \(t_{m}\) on which the 
division takes place. The Output \(G(Q_{m},\theta)\) is mathematically defined as the following \cite[10.10.7]{sklearn Decision Trees}: 

\(G(Q_{m},\theta) = \frac{N^{left}_{m}}{N_{m}} H(Q^{left}_{m}, \theta ) + \frac{N^{right}_{m}}{N_{m}} H(Q^{right}_{m}, \theta ) \)

The quality of the split is defined using an Impurity Function \(H\). The Impurity Function has one 
Leaf \(Q^{left}_{m}\) or \(Q^{right}_{m}\) and the spliting criteria \(\theta \) as its input. The 
best overall Gain \(G(Q_{m},\theta)\) is reached, if \(G(Q_{m},\theta)\) is minimized (3) \cite[10.10.6]{sklearn Decision Trees}. 

\(\theta ^* = argmin_{\theta}  G(Q_{m}, \theta)\)

The most common Impurity Function \(H\) for Classification is Gini Index (4) which is also used for CART
Decision Trees. Gini index measures the probability that a sample does not belong to the category
that represents the majority of the subgroup \cite[p.335]{James2021}. If both categories of a subgroup are identical in 
size, the Gini Index reaches its maximum point at \(0,5\) (5). The maximum of the Gini index means 
the worst possible data constellation for a subgroup. Gini index equal to \(0\) on the other 
hand represents the best possible result with the subgroup being fully homogenous. \(p_{i}\) represents that 
probability that a sample belongs to the class \(j\).

(4) \(Gini = 1 - \sum ^k_{j = 1}(p_{j})^2\)

(5) FORMULA AS A GRAPH

For the example the splits look like the following. The split of numerical features is more 
complicated as it is for categorical or binary features since it can take place at 
any value within the value range. Therefore, each value of the sample could be a possible 
threshold. To achieve the best overall Gini Index, it must be calculated for every possible 
threshold of every feature. The calculation below only shows the best possible splits for 
each feature according Gini Indexes. With \(G\) calculated for both features, the first split 
can be determined. When comparing both features it is visible that Danceability minimizes \(G\)
more than Acousticness does and therefore is according to (3) determined as the overall best possible split for 
the Root Node. 

\textbf{Acousticness:}

\(Gini^{left} = 1 - ((\frac{2}{5})^2 + (\frac{3}{5})^2) = 0,48 \)

\(Gini^{right}  = 1 - ((\frac{3}{3})^2 + (\frac{0}{3})^2) = 0 \)

\(G(Q_{m},\theta) = \frac{5}{8} * 0,48 + \frac{3}{8} * 0 = 0,30 \)

\textbf{Danceability:}

\(Gini^{left}  = 1 - ((\frac{4}{4})^2 + (\frac{0}{4})^2) = 0 \)

\(Gini^{right} = 1 - ((\frac{1}{4})^2 + (\frac{3}{4})^2) = 0,36 \)

\(G(Q_{m},\theta) = \frac{4}{8} * 0 + \frac{4}{8} * 0,36 = 0,18 \)

The splitting process is repeated for each subgroup until a stop-criteria is met. The natural 
stop criterion is a completely homogeneous dataset for a node and can also be found in the 
example for Leaf X. Such Internal Nodes automatically become Leaves and represent the category 
of samples. Other stop criteria can be predefined depending on the Algorithm used for 
implementation \cite[p.7]{lewis2000introduction}. The most relevant criterion is the definition of a maximum depth of the Decision 
Tree. Other hyperparameters are discussed in the implementation chapter. 

Tree optimization plays a very relevant role because Trees often overclassify training data 
without countermeasures \cite[p.7]{lewis2000introduction}. Although the training data is well classified, overfitted Decision 
Trees often produce very poor results for test data. Too accurate classification of training 
data can negatively affect Decision Trees, as they are less able to generalize the learned 
knowledge. Pruning is a technique used to overcome overfitting problems by reducing the size of 
Decision Trees \cite[p.331]{James2021}. Sections that provide little to no classification benefit are removed or not 
constructed during the recursive splitting process. In essence, worse results for training data 
are traded for better results for unknown data.

The experimental dataset is not large enough to take appropriate countermeasures. The complete 
Classification Tree is shown in figure X. For the second iteration, only the left subgroup was 
further split. The result are two fully homogeneous sub-subgroups created from the data Nm of the 
left subgroup. Because only two features were used, each split can be visualized using a 
coordinate system (figure X2). the colored areas present the respective splits.

\subsubsection{Evaluation of Decision Trees}

In conclusion, Decision Trees can be assessed as follows. Starting with the advantages,
the main benefit is the overall simplicity of Decision Trees, both from a technical and 
business point of view. For researchers and developers, Trees are easy to construct, require little
to no data preparation, are almost univerally applicable with a possibility of validation. 
However, the simplicity for business should not be underestimated either. When comparing Machine
Learning Algorithms, the main comparison is often the accuracy of a model. The areas in which 
Decision Trees stand out include visualization and comprehensibility. The Decision Tree Algorithm 
is a white box model that allows complete transparency and explainability \cite[p.339]{James2021} \cite[10.10.]{sklearn Decision Trees}. 

The disadvantages of Decision Trees are again closely related to its simplicity. Overfitting and 
the relative instability of Decision Trees are the main drawbacks and result in good momorization 
but a comparatively weak generalization ability \cite[p.339]{James2021} \cite[10.10.]{sklearn Decision Trees}.

CROSS VALIDATION: Cross Validation is a method structure the dataset for modelling. It allows the use of 
one single dataset for both training and validation as the dataset is randomly split into N sections. Each 
section contains an equal distribution of label data as the original dataset. One subset is reserved as a 
validation dataset while the other subsets are used for the modelling of the dataset. The modelling takes place 
N times and for every iteration another subset is used for validation. Therefore N models are created in total. 
Each model can agian be tested against the subset that is reserved for validation with the best model chosen \cite[p.8f]{lewis2000introduction}. 

\subsection{Gradient Boosting}

The Gradient Boosting Algorithm is derived from Gradient Boosting Machines, which are a family of 
powerful Machine Learning Algorithms with a certain procedure pattern for the creation of models. 
In general, GBMs are very flexible in their characteristics with the possibility of utilizing 
multiple different Machine Learning Algorithms as their foundation.

Boosting differs from classical approaches as it does not consist out of a single predictive 
model but an ensemble approach. Ensemble Algorithms contain multiple Weak Learners that form a 
committee to create a strong prediction. Weak Learners are often very simple forms of traditional 
Algorithms, like Decision Trees, and must just be able to predict parts of the dataset correctly. 
Only the combination of many Weak Learners allows the model to perform overall accurate 
predictions. The most common form of Ensemble Algorithms are Bagging Algorithms with Random 
Forests as an example. Bagging, in essence, is the combination of multiple unique models. The 
prediction is formed by aggregating the outputs from all models into a single representative 
value. Typically, all models are derived from a single Algorithm, like Decision Trees for Random 
Forests, but technically there is no limitation to aggregate outputs from different Algorithms. 
This is also the case for Boosting. 

Boosting, on the other hand, follows a different principle and does not rely on independent 
models with an aggregation function. Boosting fits new models sequentially and can thereby use 
earlier acquired knowledge for further iterations. This allows GBMs to train specific areas of 
the dataset where it has previously performed poorly \cite[p.345f]{James2021}.  

\subsubsection{Gradient Boosting Algorithm}
\label{sec:Gradient Boosting Algorithm}

(1) GRADIENT BOOSTING ALGORITHM 

The generic gradient boosting algorithm is shown in Figure X. It follows a sequence of three 
distinct steps, with step two being performed for each iteration. At the beginning, an 
additional initiation of the dataset and a Loss Function is necessary. The mathematical 
representation of the dataset is like the one used for Decision Tees. A summary: \(x_{i}\) present the 
explanatory features while \(y_{i}\) represents the corresponding label for one data point of the input 
dataset \(N\) with a total number of \(n\) samples. 

The mathematical goal of the Algorithm is to reconstruct the unknown functional dependance \(f\) 
between \(x_{i}\) and \(y_{i}\) with an estimate \(\hat{f} (x)\) for every data point, such that the specific Loss 
Function \(\Psi (y,\theta )\) is minimized (1) \cite[p.1189]{Friedman_2001} \cite[2.1]{Natekin2013}. 

(1) \(F^{*} = \arg \min_{F} L(y, F(x))\)


The Loss Function is an indicator for the quality of the model. A small Loss for a data point 
means that the prediction is close or identical to the observed Label and the model therefore 
categorizes the sample correctly whereas a high Loss implies that the model could not predict 
the sample well. Given a particular learning task and dataset, different Loss Functions must 
be considered as Loss Functions are only suitable for specific data and task constellations. The most common 
Loss Function for Binary Classification is the so-called Bernoulli Loss (2). The Bernoulli Loss 
can be transformed into a log(odds)-prediction (3) as it is better suited for further calculations. 
Variations of (3) will be used in the following section to demonstrate the Gradient Boosting 
Procedure \cite[3.1]{Natekin2013}. 

(2) BERNOULLI LOSS 

(3) BERNOULLI LOG(ODDS)-PREDICTION 

Additionally, a Machine Learning Algorithm must be defined as a Weak Learner. For GMB there 
are multiple Leaners to choose from, again the choice is mostly depending on the prediction 
task and available data \cite[3.2]{Natekin2013}. A classical approach is the use of Decision Trees, which was also 
chosen as the Weak Learner for this project. Decision Trees used for Gradient Boosting are 
always Regression Trees, regardless of whether they are used for Regression or Classification 
Problems. The optimization parameters are almost identical as for standalone Decision Trees, 
but the Trees often look very different because they are specifically created as Weak Learners. 
As a result, the Decision Trees often only consist out of very few layers with only 8-32 Leaves.

\textbf{Algorithm Procedure}

To showcase the Gradient Boosting Algorithm the same sample dataset is used as for Decision Trees. 
It again consists out of 8 samples with two features and two categories as target Labels. 

DATASET 

The first step 1) is to set an initial prediction for all samples of the dataset. The initial 
prediction is not unique for individual samples but a uniform value. The optimal initial 
prediction can be calculated using the following equation (4). For \(F_{0}(x)\), representing the 
initial prediction, a minimum of \(\gamma \) is searched for. The right-hand side of the equation 
only consists out of a sum for each sample \(i\) (of the total dataset \(N\)) of the known Loss Function 
with the respective Label \(y_{i}\) and \(\gamma \) as its input. To find the low point of the equation, 
the derivate of the Loss Function is required. The final calculation of the overall \(\log (odds)\) 
that as song is classified as “hiphop” is the \(\log_{e}\) of the sum of songs of the category “hiphop” 
divided by the sum of the songs of the category “jazz” (4). The result can be checked graphically 
as is equal to the \(x1\)-coordinate value of the low point of the \(\log (odds)\)-prediction (5). 

(4) \(F_{0}(x) = \arg \min_{\gamma } \sum_{i= 1}^{n} L(y_{i}, \gamma) \)

\(F_{0}(x) = \log_{e}(\frac{5}{3}) = 0,51\)

(5) COORDINATE SYSTEM

With the completion of step 1), one can start with step 2) of the algorithm. 2) is a sequential 
process of constructing the Regression Trees with a total of \(M\) iterations. The first iteration 
starts with \(m = 1\). 

First, the log(odds)-prediction must be converted back into a probability \(p\) with the help of a 
Logistic Function as probability is easier to use for Classification (6). The result is 
that all songs have the probability of \(0,63\) to belong to the category “hiphop”.

(6) \(p = \frac{e^{\log_{e}(odds)}}{1 + e^{\log_{e}(odds)}} = \frac{e^{\log_{e}(\frac{5}{3})}}{1 + e^{\log_{e}(\frac{5}{3})}} = 0,63\)

In a) the Pseudo Residuals \(r_{im}\) for each sample \(i\) of the dataset are created. The equation (7) 
for calculating the PR consists out of known fragments. For every sample \(i\) a PR \(r_{im}\) 
is calculated using the derivative of the \(log(odds)\)-prediction with the Label \(y_{i}\) and the 
Prediction of the last iteration \(F = F_{m - 1}\) as its input. Again, the equation can be 
transformed into a very simple equation (8). For each sample the PR can be calculated by only 
subtracting the previously calculated probability \(p\) from the observed Label \(y\). Ideally, an additional 
column is created in which the PRs are temporarily stored (figure 9). 

(7) \(r_{im} = - (\frac{\partial L(y_{i}, F(x_{i}))}{\partial F(x_{i})})_{F = F_{m - 1}} \)

(8) \(r_{im} = (y_{i} - p_{i})\)

FIGURE PSEUDO RESIDUALS 

b) constructs the Regression Trees put of the features of the samples with the corresponding 
PR as the label. For this example, only Tree Stumps are created to simplify the implementation. 
The Regression Tree for the first iteration is shown in Figure 10. After the completion of the 
Tree, Terminal Regions \(R_{jm}\) must be defined for every Leaf. \(j\) starts with 1 and is increased for 
every Leaf \cite[p.1195]{Friedman_2001}. 

FIGURE REGRESSON TREE 

Following the completion of the Regression Tree, Output Values \(\gamma_{j, m}\) are calculated by using 
the equation presented in (11). For each Leaf in the Tree \(\gamma_{j, m}\) is computed by finding 
\(\gamma_{j, m}\) that minimizes the Loss Function (1ß). Like for the initialization step, the derivative has 
to be created and must set equal to \(0\). And again, after a complicated transformation, a very simple 
equation remains (12). The \(\gamma_{j, m}\)  can be calculated using only the PR and the most 
recent predicted probabilities \(p\) for all samples in the Leaf (11). 

(10) \(\gamma_{jm} = \arg \min_{\gamma}\sum_{x_{i} in R_{ij}} L(y_{i},F_{m-1}(x_{i} + \gamma)) \)

(11) \(\gamma_{jm} = \frac{ \sum r_{im}}{\sum p_{i} * (1 - p_{i})} \) 

Part d) marks the end of the first iteration and creates a new prediction \(F_{m}(x)\) for each sample. 
The new \(log(odds)\)-prediction is based on the last \(log(odds)\)-prediction plus the Learning Rate \(\nu\) multiplied by 
the Output Value(s) for the sample of the last Regression Tree (14) \cite[p.1203]{Friedman_2001}. Normally, there is only one 
Output Value for a sample which makes the summation sign obsolete. The Learning Rate \(\nu\) is a 
hyperparameter for Gradient Boosting. For this example, a high \(\nu \) is used to better visualize 
the changes. In practice a \(\nu \) in the order of 0.1 is common as decreasing the Learning Rate tends to
give better results \cite[p.1206]{Friedman_2001}. 

(13) \(F_{m}(x) = F_m{x-1} + \nu * \sum_{j = 1}^{J_{m}} \gamma_{jm}I(x in R_{jm}) \)

Step 2 is repeated until \(M\) is reached. \(M\) marks the completion of the training of the Gradient 
Boosting Model. The \(F_{M}(x)\) is the final prediction for every sample. Lastly, the predicitions
must again be transformed to probabilites like for the calculation of the PR. For the final probabilities 
thresholds are used to compute the category to which the samples belong \cite[p.1204]{Friedman_2001}. A typical threshold 
for binary Classification is 0.5 as it splits \(F_{M}(x)\) into two equal classes. This process also 
takes place for unknown samples as described in step 3). 

Step 3) is the final step for Gradient Boosting and classifies unknown datasets. An unknown sample gets 
initialized by \(F_{0}(x)\) and is sequentially routed through every model. For each iteration the 
Prediction is updated using the Output Value of the Regression Tree. Finally, the last probability 
is used to classify the sample using the predefined threshold. 

\subsubsection{Evaluation of Gradient Boosting}

Gradient Boosting is a very powerful method as it can effectively capure complex dependencies for various Machine 
Learning Problems. GBMs enhance already existing Machine Learning Algorithms by solving many problems of single model
approaches by relying on a sequence of less complex models. This approach on the other hand comes with its own disadvantages.

The main benefit over Decision Trees is the stability of Gradient Boosting. While large trees always have to make tradeoffs 
between detail and overcategorization, gradient boosting can gradually get to a deeper and deeper level of detail thanks to 
small trees with overall better generalization. Furthermore the flexbility of Gradient Boosting and Boosting in general is massive 
as it only represents the framework with many parameters to adapt the algorithm very specifically to the usecase.  

The drawbacks of Gradient Boosting often arise in practice. Gradient Boosting has a significantly higher memory consumption and 
build time as the model must be constructed sequentially. Also the evalutation is more time consuming as the sample must be processed 
by each model. From a business perspective Gradient Boosting also has its disadvantages. While the prediciton is better, it is much 
more complex to evaluate the model and explain the results \cite[7.2]{Natekin2013}. 