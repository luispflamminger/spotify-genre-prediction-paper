\subsection{Classification in the Context of Big Data}

%Big Data is an umbrella term used to describe various technological but also organizational 
%developments. Originally, Big Data refers to large sets of structured and unstructured data
%which must be stored and processed to gain business value. Today, Big Data is also often used 
%as buzzword to outline countless modern use cases that deal with large amounts of data \cite[p.5]{Meier_2021}. Big Data 
%is therefore often used in conjunction with other keywords like automatization, personalization 
%or monitoring. This chapter presents the foundation of Big Data and gives an overview of 
%technological and business standards. (QUELLE) 
%
%\subsubsection{Relevance of Data}
%
%Data in combination with Business Intelligence has become increasingly important
%over the past decades and is closely associated with the advances of the internet
%itself \cite[p.1165]{Chen_2012}. Looking back, Business Intelligence can be divided into three sub-categories,
%which follow another linearly. The first phase is centered around getting critical
%insights into operations from structured data gathered while running the business
%and interacting with customers. Examples would be transactions and sales.
%The second phase focuses increasingly on data mining and gathering customer-specific
%data. These insights can be used to identify customer needs, opinions and interests.
%The third phase, often referred as Big Data, enhances the focus set in phase
%two by more features and much deeper analysis possibilities.
%It allows organizations and researchers to gain critical information such as location, person,
%context often through mobile and sensor-based context \cite[p.1166]{Chen_2012}. 

%In conclusion, organizations require Business Intelligence as it allows them to gain
%crucial insights which is needed to run the business and achieve an advantage
%over the competition. It is important to minimize the uncertainty of decisions
%and maximize the knowledge about the opportunity costs and derive their intended impacts. 
%It is clearly noticeable that the insights and analysis possibilities become
%progressively deeper and much more detailed.
%Along this trend the amount of data required becomes larger and larger with
%increasingly complex data structures. Size, complexity of data and deep analysis
%form the foundation of Big Data and can be found again in the 5V matrix of Big Data. 

%\subsubsection{The 5V Matrix for Big Data}
%When describing Data, a reference is often made to the five Vs,
%which highlight its main characteristics.
%The previous aspects of Big Data can again be recognized in averted form. 
%
%\textbf{Volume:} The size of the datasets is in the range of tera- and zettabyte. 
%This massive volume is not only a challenge for storing but also extracting
%relevant information \cite[p.6]{Meier_2021}. 
%
%\textbf{Variety:} Variety refers to the diversity of the data itself. For modern Business 
%Analytics almost every data format and type plays a vital role. They range from the more classical 
%text and figures to images, audio and video \cite[p.6]{Meier_2021}. Whereas classical formats 
%typically are stored in a structured way other formats rely on a semi- or unstructured database. 
%The main differences between structured and unstructured data will be discussed in the following 
%chapter as part of the different storage solutions for Big Data. Without much preface, 
%unstructured data is more difficult to classify and further complicates the extraction of 
%information but allows much deeper analysis possibilities \cite[p.2f]{Tanwar2015}.
%
%\textbf{Velocity:} Velocity is about the speed in which the data must be stored, 
%and valuable information extracted. In a fast-paced environment, like the current globalized world, 
%faster analysis can be a key advantage. Some special use cases, like malfunction detection, even 
%require real-time processing of data \cite[p.6]{Fasel_2016}. 
%
%\textbf{Value:} The goal of Business Analytics and the extraction of information out of data is, 
%as mentioned already in the previous, to create business value (Big Data Analytics, 6). Value 
%that minimizes uncertainty of action or processes and gives the operators a key advantage \cite[p.6]{Meier_2021}. 
%
%\textbf{Veracity:} Veracity describes a challenge of analytics with data. The gathered data is 
%often vague and not concise. Its information is not easily identifiable from the outside. 
%Furthermore, some samples of the dataset are often of bad quality for multiple possible reasons 
%and therefore hinder the algorithm and it’s training rather than supporting it. The predictions 
%on the other hand must be precise. This conflict is a massive challenge when creating analysis 
%models \cite[p.6]{Fasel_2016}.

%\subsubsection{Differenciation between Big Data and Big Data Analytics}
%
%The term Big Data Analytics is used in literature to describe the subcategory of Big Data that 
%focusses primarily on the analytics of existing data, as the name already suggests. The analysis 
%of data is of very high interest for many use-cases and organizations as its outcome is business 
%value (comparison to the previous chapter: Why is data so important).

The project falls under the umbrella term Big Data. Big Data is defined as a large set of information
which is stored and processed to create business value. The analysis represents a substantial part of Big Data and is 
referred to as Big Data Analytics. 

The term analytics is used to describe a systematical analysis of data of any format. Analytics is 
about detecting hidden patterns, clusters and meaningful features ranging from simple detection 
to deep analysis and predictions \cite[p.2]{Tanwar2015} \cite[p.8f]{Meier_2021}.
The creation of value is closely linked to 
the use cases for which in-depth knowledge is required. Generally, literature distinguishes 
between four subsections of analytics by grouping them according to the methodology 
and their objectives. Descriptive analytics is the simplest 
form of analytics and is often the first step for further research. It gives 
information about what has happened in the past and is mostly used for reporting. Diagnostic analytics is based on descriptive 
analytics but serves a different goal, since its objective is to give insights on why something has 
happened. Diagnostic analytics evaluates the impact of features, detects correlations 
and dependencies. Predictive analytics takes this approach one step further as it predicts what 
is most likely going to happen in the future based on the knowledge gained from past data. It 
creates models with the help of algorithms to determine the probability of outcomes. The final 
step of analytics is prescriptive analytics which is again based on its predecessor. Prescriptive 
analytics predicts outcomes and recommends actions to avoid or support them respectively \cite[p.8f]{Meier_2021}.

This project contains elements of descriptive, diagnostics and predictive analytics. Using the 
\ac{CRISP DM} process model, the dataset is analyzed first. Afterwards a predictive model 
is generated to classify new data based on known patterns.

Big Data Analytics covers a large variety of research areas that differenciate mostly in their input
and learning methodology. 

Input data exists in various formats for which adequate storage solutions and machine learning 
algorithms are required. Conventional data is almost exclusively stored in structured data formats 
while modern developments increasingly rely on semi- and unstructured data. Unstructured data has no
fixed format, scheme or structure. Structured data, on the other hand, has a fixed format and fits into 
a predefined data models. Semi-structured data is an intermediate stage and is not based on a strict
standard. It often consists of predefined tags like \ac{JSON}. This project utilizes semi-structured 
data during the collection-phases which is converted into a structured format for the modeling.

The fundamental learning methods are unsupervised, supervised and reinforced learning with 
semi-supervised learning as a subcategory. The methods differ in how the model learns and which 
input data is required \cite[4]{2018VDMAQuick}. For supervised learning, a model is created, which 
maps input values, often refered to as targets, to a known label value. The model thus learns 
on already correct and complete data through the detection of hidden pattern and correlations. 
The target labels are either nominal or numerical which results in a classification or regression 
task \cite[46]{Paass2020}.
Unsupervised learning forms the opposite as its input data has no target labels. The algorithm 
is autonomous in deriving or recognizing common structures \cite[97]{schacht2019blockchain}. Semi-supervised learning is set
between both extremes and includes samples from both categories. Reinforcement learning is 
a fundamentally different learning approach with no training data being available at the 
beginning \cite[7]{2018VDMAQuick}. The algorithm acquires data only by interacting with the environment. Models train by 
tackling challenging problems and having their decisions immediately rewarded if successful or 
punished if unsuccessful through feedback signals \cite[98]{schacht2019blockchain}.
This project uses a Gradient Boosting Algorithm, which belongs the category of supervised learning methods.
Therefore, the dataset consists of features and labels.

%Furthermore the objective of the model can be divided into the most common targets of classification 
%and regression. In classification problems, the algorithm must assign a sample to a nominal number of labels. 
%A regression is present if the label has metric target values\cite[46]{Paass2020}. The objective for this 
%project is to assign songs to one of a total of three genres. Accordingly, a classification problem is 
%present. 

%\subsubsection{Data Formats}
%
%As part of Big Data, the storage of data faces similar challenges as Big Data itself. Adequate 
%storage solutions are key to providing data for the following analysis step. This chapter gives 
%a quick overview on data types. Storage is a very important topic for Big Data but plays only a 
%minor role for the project itself and therefore will not be discussed in more detail.  
%
%Variety already outlined the shift from structured to unstructured data formats. Structured data 
%has a fixed format and fits into a predefined data model which can be stored in tabular form. 
%Unstructured data, on the other hand, has no fixed format, schema or structure.  It comes in 
%almost every form such as PDF, text, image, audio and many more. Basically, the whole internet 
%and everything that is published on it is some form of unstructured data. Therefore, it is 
%believed, that approximately 95 percent of all data is in unstructured form. In between structured 
%and unstructured data there exists a subcategory called semi-structured data. Semi-structured 
%data has no strict standard and but can be read by machines since it often consists out of 
%user-defined data tags. An example for semi-structured data is XML \cite[p.2f]{Tanwar2015}. 
%
%Structured and semi-structured data is relatively easy to analyze compared to unstructured data 
%because machines mostly rely on structural organization. Unstructured data, on the contrary, has 
%great potential because the amount of information stored inside it is huge. To analyze 
%unstructured data, it is often required to deconstruct it into metadata which again is comparable 
%to semi-structured data. Often both structured and unstructured data is necessary to form 
%well-founded business decisions and gain a competitive advantage \cite[p.2f]{Tanwar2015}. 
%
%The dataset used for this project and described in more detail in chapter X is a structured 
%dataset. It consists of classical numerical and categorical features which all have a predefined 
%range of values. 
%
%\subsubsection{Categories of Machine Learning}
%
%Machine learning refers to the ability of a computer to learn on its own.
%The field of machine learning includes a wide range of algorithms that learn
%from data and make predictions with variable quality.
%These predictions are not made programmatically but by data-driven predictions
%that are "learned" by generating knowledge. Basically,
%Machine Learning can be divided into 3 different methods which are discussed below.\cite[4]{2018VDMAQuick}
%
%\textbf{Supervised Learning}
%In supervised learning, a function is determined by mapping input values to known target
%values using examples. Supervised learning is also called learning from examples for this reason.\cite[96]{schacht2019blockchain}
%The system learns based on a training data set that already contains the correct answers.
%Using the already given data sets, the algorithm learns to set up rules and patterns to
%reach the known target variable. The process is repeated until the prediction matches the desired quality.
%Experiences from each iteration are in turn included in the learning process.
%If the trained model fulfills the desired results, it can also be applied to unknown data.
%The goal of this learning method is therefore to make predictions and recommendations.\cite[96]{schacht2019blockchain}
%It is also important to mention that target values in machine learning are called labels.
%Here again, one must distinguish between two use cases. If the label is nominal, it is a so-called
%classification. So, the model should assign data to specific classes.
%A regression is present if the label has metric target values.\cite[46]{Paass2020}
%The aim regression is to use data to make forecasts of future values or to identify trends.
%Problems that can occur when using supervised learning and should be avoided if possible
%are either overfitting or underfitting.  Overfitting is when the algorithm is adapted too much
%to the training data.
%This means that it delivers better results when applied to training data than to unknown data.
%The opposite is called underfitting which means the models of the learning procedure are not complex
%enough.
%Therefore, the algorithm cannot deliver sufficient performance to make predictions.\cite{buxmann2018künstliche}
%
%\textbf{Unsupervised Learning}
%The opposite of supervised learning is unsupervised learning.
%Here, the algorithm itself acquires patterns and correlations without explicitly
%predefining target values.
%Which is why the algorithm is also called learning from observations.\cite[97]{schacht2019blockchain}
%This is also done using sample data, but without labeled output data as in supervised learning.
%The algorithm thus focuses on deriving or recognizing common structures and patterns in the sample data.\cite[7]{2018VDMAQuick}
%The search includes the complete training data and not only relations with concrete
%target values as in supervised learning.\cite[802]{ernst2016grundkurs}
%Unsupervised learning can again be divided into several types.
%Clustering, for example, deals with finding frequency structures, patterns and grouping the
%data into a few sets.\cite[260]{Ertel2021}
%Another type of learning are associations which search for rules that map connections between data points.
%Finally, there is also dimensionality reduction. This kind of learning tries to reduce
%all available variables to the most important ones.\cite[10]{FraunhoferMasch2018}
%The problem with unsupervised learning, however, is that no conclusions can be drawn about
%the quality of the algorithm due to the non-existence of target variables.
%Thus, there is no real right or wrong in the learning process and so the algorithm has the possibility
%to fail completely.\cite[97]{schacht2019blockchain}
%
%\textbf{Semi Supervised Learning}
%Semi-supervised learning is a learning method that includes elements of supervised
%and unsupervised learning.
%Because of this combination, it is often not considered a separate form of learning,
%but it is important to mention it. This method uses training data that are only partially labeled.\cite[98]{schacht2019blockchain}
%This training data serves as a representative label of discovered structures if unsupervised learning is used.
%If supervised methods are used, the unlabeled data serves to make statistical accumulations more estimable.
%The main advantage of semi-supervised learning is that
%training of algorithms is already possible with little data.
%This reduces the high costs and the effort that often have to be spent on target values.\cite{WuttkeDatasolutMachine}
%
%\textbf{Reinforcement Learning}
%Reinforcement learning is fundamentally different from the learning methods already mentioned.
%Unlike the learning methods already discussed, no training data is available at the beginning.\cite[351]{Ertel2021}
%The algorithm acquires data only by interacting with the environment,
%which is why this learning method can also be called learning by interaction.
%Due to the few Requirements, this learning method is optimally suited\cite[98]{schacht2019blockchain}
%Models train by tackling challenging problems and having their decisions immediately rewarded
%if successful or punished if unsuccessful through feedback signals.
%This reward or punishment occurs, for example, in chess when a game is won or lost.
%Moves that lead to victory are saved, as are moves that lead to a possible loss.\cite[98]{schacht2019blockchain}
%In contrast to other learning methods, the algorithm does not know whether a decision is right or
%wrong before it decides what to do next.
%In addition, it does not know whether the decision it is currently making is the best one for the situation at hand,
%since its wealth of experience only grows with increasing runtime. In order to cover all possible situations,
%the algorithm must initially also cover previously unknown possibilities and not only act on the basis of actions
%that have led to reward or punishment in the past.
%However, if the algorithm is trained well enough, it can solve problems very well on this basis.
%Overall goal is to develop a strategy to maximize rewards received and get better as fast as possible.\cite[351]{Ertel2021}