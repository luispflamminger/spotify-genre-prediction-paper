\subsection{Basic Concepts of Big Data}

Big Data is an umbrella term used to describe various technological but also organizational 
developments. Originally, Big Data refers to large sets of structured and unstructured data
which must be stored and processed to gain business value. Today, Big Data is also often used 
as buzzword to outline countless modern use cases that deal with large amounts of data \cite[p.5]{Meier_2021}. Big Data 
is therefore often used in conjunction with other keywords like automatization, personalization 
or monitoring. This chapter presents the foundation of Big Data and gives an overview of 
technological and business standards. (QUELLE) 

\subsubsection{Relevance of Data}

Data in combination with Business Intelligence has become increasingly important
over the past decades and is closely associated with the advances of the internet
itself \cite[p.1165]{Chen_2012}. Looking back, Business Intelligence can be divided into three sub-categories,
which follow another linearly. The first phase is centered around getting critical
insights into operations from structured data gathered while running the business
and interacting with customers. Examples would be transactions and sales.
The second phase focuses increasingly on data mining and gathering customer-specific
data. These insights can be used to identify customer needs, opinions and interests.
The third phase, often referred as Big Data, enhances the focus set in phase
two by more features and much deeper analysis possibilities.
It allows organizations and researchers to gain critical information such as location, person,
context often through mobile and sensor-based context \cite[p.1166]{Chen_2012}. 

In conclusion, organizations require Business Intelligence as it allows them to gain
crucial insights which is needed to run the business and achieve an advantage
over the competition. It is important to minimize the uncertainty of decisions
and maximize the knowledge about the opportunity costs and derive their intended impacts. 
It is clearly noticeable that the insights and analysis possibilities become
progressively deeper and much more detailed.
Along this trend the amount of data required becomes larger and larger with
increasingly complex data structures. Size, complexity of data and deep analysis
form the foundation of Big Data and can be found again in the 5V matrix of Big Data. 

\subsubsection{The 5V Matrix for Big Data}
When describing Data, a reference is often made to the five Vs,
which highlight its main characteristics.
The previous aspects of Big Data can again be recognized in averted form. 

\textbf{Volume:} The size of the datasets is in the range of tera- and zettabyte. 
This massive volume is not only a challenge for storing but also extracting
relevant information \cite[p.6]{Meier_2021}. 

\textbf{Variety:} Variety refers to the diversity of the data itself. For modern Business 
Analytics almost every data format and type plays a vital role. They range from the more classical 
text and figures to images, audio and video \cite[p.6]{Meier_2021}. Whereas classical formats 
typically are stored in a structured way other formats rely on a semi- or unstructured database. 
The main differences between structured and unstructured data will be discussed in the following 
chapter as part of the different storage solutions for Big Data. Without much preface, 
unstructured data is more difficult to classify and further complicates the extraction of 
information but allows much deeper analysis possibilities \cite[p.2f]{Tanwar2015}.

\textbf{Velocity:} Velocity is about the speed in which the data must be stored, 
and valuable information extracted. In a fast-paced environment, like the current globalized world, 
faster analysis can be a key advantage. Some special use cases, like malfunction detection, even 
require real-time processing of data \cite[p.6]{Fasel_2016}. 

\textbf{Value:} The goal of Business Analytics and the extraction of information out of data is, 
as mentioned already in the previous, to create business value (Big Data Analytics, 6). Value 
that minimizes uncertainty of action or processes and gives the operators a key advantage \cite[p.6]{Meier_2021}. 

\textbf{Veracity:} Veracity describes a challenge of analytics with data. The gathered data is 
often vague and not concise. Its information is not easily identifiable from the outside. 
Furthermore, some samples of the dataset are often of bad quality for multiple possible reasons 
and therefore hinder the algorithm and itâ€™s training rather than supporting it. The predictions 
on the other hand must be precise. This conflict is a massive challenge when creating analysis 
models \cite[p.6]{Fasel_2016}.

\subsubsection{Differenciation between Big Data and Big Data Analytics}

The term Big Data Analytics is used in literature to describe the subcategory of Big Data that 
focusses primarily on the analytics of existing data, as the name already suggests. The analysis 
of data is of very high interest for many use-cases and organizations as its outcome is business 
value (comparison to the previous chapter: Why is data so important).

The term Analytics is used to describe a systematical analysis of data in some form. Analytics is 
about detecting hidden patterns, clusters and meaningful features ranging from simple detection 
over deep analysis and predictions. The goal is to process the data in such a way that value is 
created from it \cite[p.2]{Tanwar2015} \cite[p.8f]{Meier_2021}. The creation of value is closely linked to 
the use cases for which in-depth knowledge is required and in which form. Generally, literature 
distinguishes between four main groups of analytics by grouping them according to the methodology 
and their objectives starting with Descriptive Analytics. Descriptive Analytics is the simplest 
form of Analytics and is often the first step for further research. Descriptive Analytics gives 
information about what has happened in the past. Descriptive Analytics is often used for reporting 
on relevant topics for operation such as KPIs, sales or revenue. Typically, visual presentation 
methods are used for displaying the information. Diagnostic Analytics is based on Descriptive 
Analytics but serves a different goal, since its goal is to give insights on why something has 
happened in the past. Diagnostic Analytics evaluates the impact of features, detects correlations 
and dependencies. Predictive Analytics takes this approach one step further as it predicts what 
is most likely going to happen in the future based on the knowledge gained from past data. It 
creates models with the help of algorithms to determine the probability of outcomes. The final 
step of Analytics is Prescriptive Analytics, which again is based on its predecessor. Prescriptive 
Analytics predicts outcomes and recommends actions to avoid or support them respectively \cite[p.8f]{Meier_2021}.

This project contains elements of descriptive, diagnostics and predictive analytics. Using the 
Crisp Dm process model (CHAPTER X), the data set is first analyzed and then a predictive model 
is generated to classify new data based on known data.

https://www.sigmacomputing.com/blog/descriptive-predictive-prescriptive-and-diagnostic-analytics-a-quick-guide/

\subsubsection{Data Formats}

As part of Big Data, the storage of data faces similar challenges as Big Data itself. Adequate 
storage solutions are key to providing data for the following analysis step. This chapter gives 
a quick overview on data types. Storage is a very important topic for Big Data but plays only a 
minor role for the project itself and therefore will not be discussed in more detail.  

Variety already outlined the shift from structured to unstructured data formats. Structured data 
has a fixed format and fits into a predefined data model which can be stored in tabular form. 
Unstructured data, on the other hand, has no fixed format, schema or structure.  It comes in 
almost every form such as PDF, text, image, audio and many more. Basically, the whole internet 
and everything that is published on it is some form of unstructured data. Therefore, it is 
believed, that approximately 95 percent of all data is in unstructured form. In between structured 
and unstructured data there exists a subcategory called semi-structured data. Semi-structured 
data has no strict standard and but can be read by machines since it often consists out of 
user-defined data tags. An example for semi-structured data is XML \cite[p.2f]{Tanwar2015}. 

Structured and semi-structured data is relatively easy to analyze compared to unstructured data 
because machines mostly rely on structural organization. Unstructured data, on the contrary, has 
great potential because the amount of information stored inside it is huge. To analyze 
unstructured data, it is often required to deconstruct it into metadata which again is comparable 
to semi-structured data. Often both structured and unstructured data is necessary to form 
well-founded business decisions and gain a competitive advantage \cite[p.2f]{Tanwar2015}. 

The dataset used for this project and described in more detail in chapter X is a structured 
dataset. It consists of classical numerical and categorical features which all have a predefined 
range of values. 

\subsubsection{Categories of Machine Learning}

Machine learning refers to the ability of a computer to learn on its own.
The field of machine learning includes a wide range of algorithms that learn
from data and make predictions with variable quality.
These predictions are not made programmatically but by data-driven predictions
that are "learned" by generating knowledge. Basically,
Machine Learning can be divided into 3 different methods which are discussed below.\cite[4]{2018VDMAQuick}

\textbf{Supervised Learning}
In supervised learning, a function is determined by mapping input values to known target
values using examples. Supervised learning is also called learning from examples for this reason.\cite[96]{schacht2019blockchain}
The system learns based on a training data set that already contains the correct answers.
Using the already given data sets, the algorithm learns to set up rules and patterns to
reach the known target variable. The process is repeated until the prediction matches the desired quality.
Experiences from each iteration are in turn included in the learning process.
If the trained model fulfills the desired results, it can also be applied to unknown data.
The goal of this learning method is therefore to make predictions and recommendations.\cite[96]{schacht2019blockchain}
It is also important to mention that target values in machine learning are called labels.
Here again, one must distinguish between two use cases. If the label is nominal, it is a so-called
classification. So, the model should assign data to specific classes.
A regression is present if the label has metric target values.\cite[46]{Paass2020}
The aim regression is to use data to make forecasts of future values or to identify trends.
Problems that can occur when using supervised learning and should be avoided if possible
are either overfitting or underfitting.  Overfitting is when the algorithm is adapted too much
to the training data.
This means that it delivers better results when applied to training data than to unknown data.
The opposite is called underfitting which means the models of the learning procedure are not complex
enough.
Therefore, the algorithm cannot deliver sufficient performance to make predictions.\cite{buxmann2018kÃ¼nstliche}

\textbf{Unsupervised Learning}
The opposite of supervised learning is unsupervised learning.
Here, the algorithm itself acquires patterns and correlations without explicitly
predefining target values.
Which is why the algorithm is also called learning from observations.\cite[97]{schacht2019blockchain}
This is also done using sample data, but without labeled output data as in supervised learning.
The algorithm thus focuses on deriving or recognizing common structures and patterns in the sample data.\cite[7]{2018VDMAQuick}
The search includes the complete training data and not only relations with concrete
target values as in supervised learning.\cite[802]{ernst2016grundkurs}
Unsupervised learning can again be divided into several types.
Clustering, for example, deals with finding frequency structures, patterns and grouping the
data into a few sets.\cite[260]{Ertel2021}
Another type of learning are associations which search for rules that map connections between data points.
Finally, there is also dimensionality reduction. This kind of learning tries to reduce
all available variables to the most important ones.\cite[10]{FraunhoferMasch2018}
The problem with unsupervised learning, however, is that no conclusions can be drawn about
the quality of the algorithm due to the non-existence of target variables.
Thus, there is no real right or wrong in the learning process and so the algorithm has the possibility
to fail completely.\cite[97]{schacht2019blockchain}

\textbf{Semi Supervised Learning}
Semi-supervised learning is a learning method that includes elements of supervised
and unsupervised learning.
Because of this combination, it is often not considered a separate form of learning,
but it is important to mention it. This method uses training data that are only partially labeled.\cite[98]{schacht2019blockchain}
This training data serves as a representative label of discovered structures if unsupervised learning is used.
If supervised methods are used, the unlabeled data serves to make statistical accumulations more estimable.
The main advantage of semi-supervised learning is that
training of algorithms is already possible with little data.
This reduces the high costs and the effort that often have to be spent on target values.\cite{WuttkeDatasolutMachine}

\textbf{Reinforcement Learning}
Reinforcement learning is fundamentally different from the learning methods already mentioned.
Unlike the learning methods already discussed, no training data is available at the beginning.\cite[351]{Ertel2021}
The algorithm acquires data only by interacting with the environment,
which is why this learning method can also be called learning by interaction.
Due to the few Requirements, this learning method is optimally suited\cite[98]{schacht2019blockchain}
Models train by tackling challenging problems and having their decisions immediately rewarded
if successful or punished if unsuccessful through feedback signals.
This reward or punishment occurs, for example, in chess when a game is won or lost.
Moves that lead to victory are saved, as are moves that lead to a possible loss.\cite[98]{schacht2019blockchain}
In contrast to other learning methods, the algorithm does not know whether a decision is right or
wrong before it decides what to do next.
In addition, it does not know whether the decision it is currently making is the best one for the situation at hand,
since its wealth of experience only grows with increasing runtime. In order to cover all possible situations,
the algorithm must initially also cover previously unknown possibilities and not only act on the basis of actions
that have led to reward or punishment in the past.
However, if the algorithm is trained well enough, it can solve problems very well on this basis.
Overall goal is to develop a strategy to maximize rewards received and get better as fast as possible.\cite[351]{Ertel2021}


\subsubsection{Mean Removal, Variance Scaling and Standardization}
\label{sec:Mean Removal, Variance Scaling and Standardization}
During the data preparation stage of model development, it can be helpful to transform the data into a different shape
to improve the results of a model.\cite[35]{Subasi2020} This subsection explains some methods of data transformation, which are used in the
implementation section.

\textbf{Mean Removal} means shifting the data in a column, so that the mean $\overline{x}$ of all datapoints in the
column is equal to $0$.\cite{ScikitPreprocessing}
An example set of integers $a_1 = \{6, 7, 2, 1\}$ has a mean of $\overline{x_1} = 4$.
Removing it's mean without distorting other properties of the data entails subtracting $4$ from each member of the
set, resulting in a new set $a_2 = \{2, 3, -2, -3\}$ with $\overline{x_2} = 0$. The dataset is now centered around $0$.

\textbf{Variance Scaling} involves transforming the data in a way, that each column has unit variance, meaning variance
$\sigma^2 = 1$.\cite{ScikitPreprocessing} This is done by dividing each member of the column by the column's standard deviation.
An example set of integers $b_1 = \{6, 7, 2, 1\}$ has standard deviation $\sigma_1 = 2.94$ and variance $\sigma_1^2 = 8.67$.
Dividing each member of $b_1$ by $\sigma_1$ results in the set $b_2 = \{2.04, 2.38, 0.68, 0.34\}$ with variance $\sigma_2^2 = 1$.

\textbf{Standardization} involves performing both mean removal and variance scaling on a column.\cite{ScikitPreprocessing}
Standardizing an example set $c_1 = \{6, 7, 2, 1\}$ would result in the following set $c_2 = \{0.68, 1.02, -0.68, 01.02\}$.
Standardized features are normally distributed. Many machine learning have improved results when receiving standardized input
data. It has the additional benefit of equalizing the impact of features while keeping valuable information about the outliers
and value range.\cite[35]{Subasi2020}

\subsubsection{Dimension Reduction}
\label{sec:Dimension Reduction}

Dimension reduction refers to reducing the number of dimensions, so the number of
features in the input data, while keeping the highest possible amount of information from
the original dataset. The benefit of this is, that some features might not contribute to a better model,
but instead promote bias and overfitting. With feature selection, the smallest amount of features,
which offers the highest ability of generalization is attempted to be found.
There are multiple ways to do this, such as feature selection, where some features, which don't
contribute to a better model, are simply removed.
New features can be constructed by using data from different original features and
combining it into one, also reducing the dimensionality.
Another approach is \ac{PCA}. Here features are constructed by finding
principal components in a dataset. This is done by denoting the data in m-dimensional space,
where each feature is contained in one spatial dimension.
The first principal component is found by rotating the feature space to find the direction
which offers the highest variance in the whole dataset.
The second component must be orthogonal to the first one and again have the highest variance
of all possible directions. This can be repeated to create as many principal components as are benefitial to the model.
Dimension Reduction using \ac{PCA} was attempted as part of this project,
but is not explained deeper as it didn't yield great result for this dataset.

\subsubsection{Hyperparameter Optimization}

Machine learning models have two types of parameters: model parameters and hyperparameters.
Model parameters are values, which are optimized using training data during the training process.
They are not specified in advance.
Hyperparameters on the other hand are not optimized using data, but are specified before training.
They are like "settings" for the estimator. 
\textbf{\{Add example for decision trees\}}

Hyperparameters are not optimized during training, but still influence the results of a model.
There are multiple ways to find the best set of hyperparameters, most commonly randomized search
and grid search.

During grid search a parameter grid is specified, which contains multiple values for each hyperparameter.
A model is trained using every possible combination of all given values on the grid and a score
is calculated for each model. The best combination of hyperparameters are the ones used on the model
with the highest score. This might not be the optimal set of hyperparameters though, as only values
in the search grid are considered. Execution time is a result of the number of combinations and the
hardware used.

Randomized search uses the same concept, but picks random values for each hyperparameter.
As this method does not have a stop criterion and could go on forever, it is limited by specifying
an amount of time after which it stops and returns the best hyperparameters it could find in
that timeframe.

\subsubsection{k-fold Cross Validation}

When training a model, it is necessary to split the data up into train and test samples.
Usually 80\% of samples are used for training, while 20\% are used for testing.
This is necessary, because using the same data for training and calculating an accuracy
score when optimizing the model, results in a model that is fitted exactly to the training
data. It will perform very well on that data, but will fail to make predictions on any data
it has not yet seen. This is called overfitting.

When optimizing the hyperparameters of an estimator, one could use the test set to score
the model and find the best parameters. A problem with this approach is, that now test data,
which should be completely independent from the training process and only used for calculating
the final score, is involved in the optimization process, which could again lead to the model being
overfitted directly to the training and test data.

Cross validation is a solution to this problem During optimization, the training set is
split into k groups, named folds. A given estimator is trained on $k-1$ folds and is accuracy
is calculated using the samples in the remaining folds. This is done $k$ times, using a different fold
for scoring each time. The average of all scores is the cross-validation score, giving a good
representation of the overall performance of the estimator.
The benefit of this is, that test data is kept completely seperate from training data, but there
is still a good way of evaluating the performance of an estimator trained with a given set
of hyperparameters. This is crucial during hyperparameter optimization, as the best parameters
can not be found without a good way to score each model.




